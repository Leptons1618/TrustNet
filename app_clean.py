"""
TrustLens: Explainable & Trust-Aware AI
Main Streamlit Application
"""

import streamlit as st
import torch
import numpy as np
import pandas as pd
from PIL import Image
import os
import sys
import traceback

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from src.models import TrustModel
from src.utils import (
    CIFAR10DataLoader, 
    preprocess_uploaded_image, 
    tensor_to_image,
    plot_trust_metrics,
    create_trust_gauge,
    plot_confidence_calibration
)
from src.trust_methods import MahalanobisDetector, TemperatureScaling

# Try to import GradCAM
try:
    from src.utils.gradcam import create_gradcam_for_model
    GRADCAM_AVAILABLE = True
except ImportError:
    GRADCAM_AVAILABLE = False


# Page configuration
st.set_page_config(
    page_title="TrustLens AI",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        font-weight: bold;
        text-align: center;
        margin-bottom: 2rem;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }
    .metric-card {
        background: white;
        padding: 1rem;
        border-radius: 0.5rem;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        border-left: 4px solid #667eea;
    }
    .trust-high { border-left-color: #10b981 !important; }
    .trust-medium { border-left-color: #f59e0b !important; }
    .trust-low { border-left-color: #ef4444 !important; }
</style>
""", unsafe_allow_html=True)


@st.cache_resource
def load_model():
    """Load the pre-trained model"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Check for model file
    model_paths = [
        "cookbook/trustnet_cnn.pth",
        "og_Model/trustnet_cnn.pth",
        "models/trustnet_cnn.pth"
    ]
    
    model_path = None
    for path in model_paths:
        if os.path.exists(path):
            model_path = path
            break
    
    if model_path is None:
        st.error("Model file not found! Please ensure trustnet_cnn.pth exists in one of: " + ", ".join(model_paths))
        return None
    
    try:
        trust_model = TrustModel(model_path=model_path, device=device)
        st.success(f"âœ… Model loaded successfully from {model_path}")
        return trust_model
    except Exception as e:
        st.error(f"Failed to load model: {str(e)}")
        return None


@st.cache_resource
def load_data_loader():
    """Load CIFAR-10 data loader"""
    try:
        data_loader = CIFAR10DataLoader(data_dir='./cookbook/data', batch_size=32)
        return data_loader
    except Exception as e:
        st.error(f"Failed to load data: {str(e)}")
        return None


def main():
    """Main application"""
    
    # Page navigation
    st.sidebar.title("ğŸ” TrustLens AI")
    page = st.sidebar.selectbox("Navigate", ["ğŸ  Home", "ğŸ“– About", "âš™ï¸ Settings"])
    
    if page == "ğŸ“– About":
        show_about_page()
        return
    elif page == "âš™ï¸ Settings":
        show_settings_page()
        return
    
    # Main home page content
    show_main_page()


def show_about_page():
    """Display the About page"""
    st.markdown('<h1 class="main-header">ğŸ“– About TrustLens</h1>', unsafe_allow_html=True)
    
    st.markdown("""
    ## ğŸ” What is TrustLens?
    
    TrustLens is an **explainable and trust-aware AI prototype** that combines state-of-the-art uncertainty quantification 
    and explainability techniques to provide transparent and trustworthy image classification predictions.
    
    ### ğŸ¯ Key Features
    """)
    
    # Feature explanations with examples
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        #### ğŸ² Entropy-based Uncertainty
        **What it does:** Measures how confident the model is in its predictions using information theory.
        
        **How it works:** Calculates the entropy of the prediction probabilities. Lower entropy = higher confidence.
        
        **Example:** If the model predicts [0.9, 0.05, 0.05], the entropy is low (confident). 
        If it predicts [0.4, 0.3, 0.3], the entropy is high (uncertain).
        
        **Parameter Effects:**
        - **Threshold:** Lower values flag more predictions as uncertain
        - **Temperature:** Higher values make predictions more uniform (higher entropy)
        """)
        
        st.markdown("""
        #### ğŸ“ Mahalanobis Distance
        **What it does:** Detects anomalies by measuring how far an input is from the training data distribution.
        
        **How it works:** Computes distance in the feature space, accounting for correlations between features.
        
        **Example:** An image of a car when trained on animals would have high Mahalanobis distance.
        
        **Parameter Effects:**
        - **Threshold:** Higher values allow more deviation from training data
        - **Layer:** Deeper layers capture more semantic features
        """)
    
    with col2:
        st.markdown("""
        #### ğŸš¨ ODIN Detection
        **What it does:** Identifies out-of-distribution (OOD) samples using input preprocessing and temperature scaling.
        
        **How it works:** Applies small perturbations to inputs and uses temperature scaling to amplify differences.
        
        **Example:** Helps distinguish between CIFAR-10 images and completely different datasets.
        
        **Parameter Effects:**
        - **Epsilon:** Controls perturbation magnitude (typically 0.0014)
        - **Temperature:** Higher values separate in-distribution from OOD better
        """)
        
        st.markdown("""
        #### ğŸ” Grad-CAM Explanation
        **What it does:** Visualizes which parts of the image the model focuses on for its decision.
        
        **How it works:** Uses gradients to highlight important regions in the input image.
        
        **Example:** For a cat image, it might highlight the ears, eyes, and whiskers.
        
        **Interpretation:**
        - ğŸ”´ **Red areas:** Most important for the decision
        - ğŸŸ¡ **Yellow areas:** Moderately important
        - ğŸ”µ **Blue areas:** Less important
        """)
    
    st.markdown("""
    ### ğŸšï¸ Trust Score Calculation
    
    TrustLens combines multiple uncertainty measures into a single trust score:
    
    ```
    Trust Score = weighted_average(
        normalized_entropy,
        normalized_odin_score, 
        normalized_mahalanobis_score
    )
    ```
    
    **Trust Levels:**
    - ğŸŸ¢ **HIGH (>0.8):** Model is very confident and the input seems familiar
    - ğŸŸ¡ **MEDIUM (0.5-0.8):** Some uncertainty or potential anomaly detected
    - ğŸ”´ **LOW (<0.5):** High uncertainty or likely out-of-distribution input
    
    ### ğŸ”¬ Use Cases and Examples
    """)
    
    use_case_cols = st.columns(3)
    
    with use_case_cols[0]:
        st.markdown("""
        #### ğŸ¥ Medical Imaging
        - **High Trust:** Clear X-ray with obvious fracture
        - **Medium Trust:** Ambiguous scan requiring expert review  
        - **Low Trust:** Corrupted or unusual image format
        """)
    
    with use_case_cols[1]:
        st.markdown("""
        #### ğŸš— Autonomous Driving
        - **High Trust:** Clear road with standard traffic signs
        - **Medium Trust:** Weather conditions affecting visibility
        - **Low Trust:** Construction zone with unfamiliar setup
        """)
    
    with use_case_cols[2]:
        st.markdown("""
        #### ğŸ­ Quality Control
        - **High Trust:** Standard product with clear defect
        - **Medium Trust:** Borderline quality issues
        - **Low Trust:** Completely new product type
        """)
    
    st.markdown("""
    ### ğŸ› ï¸ Technical Implementation
    
    **Model Architecture:**
    - Convolutional Neural Network (CNN) trained on CIFAR-10
    - 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck
    
    **Trust Methods:**
    - **Entropy:** `H(p) = -Î£ p_i * log(p_i)`
    - **ODIN:** Input perturbation + temperature scaling
    - **Mahalanobis:** `d = sqrt((x-Î¼)áµ€ Î£â»Â¹ (x-Î¼))`
    - **Grad-CAM:** Gradient-weighted class activation mapping
    
    **Framework:** Built with PyTorch, Streamlit, and modern ML libraries
    
    ### ğŸ“š References
    
    - **ODIN:** Liang et al. "Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks" (2018)
    - **Mahalanobis:** Lee et al. "A Simple Unified Framework for Detecting Out-of-Distribution Samples" (2018)
    - **Grad-CAM:** Selvaraju et al. "Grad-CAM: Visual Explanations from Deep Networks" (2017)
    - **Temperature Scaling:** Guo et al. "On Calibration of Modern Neural Networks" (2017)
    
    ### ğŸš€ Getting Started
    
    1. **Upload an Image:** Use your own image for analysis
    2. **Try Samples:** Use pre-loaded examples to see different trust levels
    3. **Explore Settings:** Adjust parameters to see how they affect trust scores
    4. **View Explanations:** Enable Grad-CAM to see what the model focuses on
    
    Built with â¤ï¸ for transparent and trustworthy AI.
    """)


def show_settings_page():
    """Display the Settings page"""
    st.markdown('<h1 class="main-header">âš™ï¸ Settings</h1>', unsafe_allow_html=True)
    
    st.markdown("### ğŸ›ï¸ Advanced Configuration")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Trust Method Parameters")
        
        # ODIN settings
        st.markdown("#### ğŸš¨ ODIN Detection")
        odin_epsilon = st.slider("Perturbation Magnitude (Îµ)", 0.0001, 0.01, 0.0014, format="%.4f")
        odin_temperature = st.slider("ODIN Temperature", 100, 2000, 1000)
        
        st.info(f"Current ODIN settings: Îµ={odin_epsilon}, T={odin_temperature}")
        
        # Temperature scaling
        st.markdown("#### ğŸŒ¡ï¸ Temperature Scaling")
        temp_scaling = st.slider("Temperature", 0.1, 5.0, 1.0)
        
        # Thresholds
        st.markdown("#### ğŸ¯ Trust Thresholds")
        high_trust_thresh = st.slider("High Trust Threshold", 0.7, 0.95, 0.8)
        low_trust_thresh = st.slider("Low Trust Threshold", 0.3, 0.6, 0.5)
    
    with col2:
        st.subheader("Visualization Settings")
        
        # Grad-CAM settings
        st.markdown("#### ğŸ” Grad-CAM")
        gradcam_layer = st.selectbox("Target Layer", ["features.11", "features.9", "features.7"])
        gradcam_alpha = st.slider("Overlay Transparency", 0.1, 0.9, 0.4)
        
        # Display settings
        st.markdown("#### ğŸ¨ Display Options")
        show_probabilities = st.checkbox("Show All Class Probabilities", value=True)
        show_confidence = st.checkbox("Show Confidence Metrics", value=True)
        show_explanations = st.checkbox("Show Method Explanations", value=True)
        
        # Performance settings
        st.markdown("#### âš¡ Performance")
        batch_size = st.selectbox("Batch Size", [1, 4, 8, 16, 32], index=0)
        device_preference = st.selectbox("Device", ["Auto", "CPU", "CUDA"])
    
    # Save settings
    if st.button("ğŸ’¾ Save Settings"):
        st.success("Settings saved! (Note: This is a demo - settings are not persistent)")
        
    # Reset to defaults
    if st.button("ğŸ”„ Reset to Defaults"):
        st.success("Settings reset to defaults!")


def show_main_page():
    """Display the main application page"""
    
    # Header
    st.markdown('<h1 class="main-header">ğŸ” TrustLens AI</h1>', unsafe_allow_html=True)
    st.markdown("### Explainable & Trust-Aware Image Classification")
    st.markdown("---")
    
    # Load model
    trust_model = load_model()
    if trust_model is None:
        st.stop()
    
    # Load data loader
    data_loader = load_data_loader()
    
    # Sidebar
    st.sidebar.header("ğŸ›ï¸ Controls")
    
    # Image input options
    input_method = st.sidebar.selectbox(
        "Choose input method:",
        ["Upload Image", "Sample Images", "CIFAR-10 Test"]
    )
    
    # Trust method selection
    st.sidebar.subheader("Trust Analysis Methods")
    use_entropy = st.sidebar.checkbox("Entropy-based Uncertainty", value=True)
    use_odin = st.sidebar.checkbox("ODIN Detection", value=True)
    use_mahalanobis = st.sidebar.checkbox("Mahalanobis Distance", value=False)
    use_gradcam = st.sidebar.checkbox("Grad-CAM Explanation", value=GRADCAM_AVAILABLE)
    
    if use_gradcam and not GRADCAM_AVAILABLE:
        st.sidebar.warning("Grad-CAM requires opencv-python. Install it to enable this feature.")
        use_gradcam = False
    
    # Advanced settings
    with st.sidebar.expander("âš™ï¸ Advanced Settings"):
        confidence_threshold = st.slider("Confidence Threshold", 0.0, 1.0, 0.8)
        temperature = st.slider("Temperature Scaling", 0.1, 5.0, 1.0)
      # Main content area
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("ğŸ“· Input Image")
        
        input_tensor = None
        original_image = None
        
        if input_method == "Upload Image":
            uploaded_file = st.file_uploader(
                "Choose an image...", 
                type=['png', 'jpg', 'jpeg'],
                help="Upload an image for classification and trust analysis"
            )
            
            if uploaded_file is not None:
                original_image = Image.open(uploaded_file)
                st.image(original_image, caption="Uploaded Image", use_container_width=True)
                input_tensor = preprocess_uploaded_image(original_image)
        
        elif input_method == "Sample Images":
            # Create sample images if data loader is available
            if data_loader:
                st.write("Select a sample to immediately display and analyze:")
                
                # Create columns for sample selection
                sample_cols = st.columns(5)
                
                sample_data = []
                for i in range(5):
                    with sample_cols[i]:
                        if st.button(f"Sample {i+1}", key=f"sample_{i+1}"):
                            try:
                                data_loaders = data_loader.get_loaders()
                                test_dataset = data_loaders['datasets']['test_clean']
                                
                                # Use fixed seed for consistent samples
                                np.random.seed(i * 123)  # Different seed for each sample
                                idx = np.random.randint(0, len(test_dataset))
                                sample_tensor, sample_label = test_dataset[idx]
                                
                                # Store in session state for persistence
                                st.session_state[f'sample_{i+1}_tensor'] = sample_tensor
                                st.session_state[f'sample_{i+1}_label'] = sample_label
                                st.session_state['selected_sample'] = i+1
                                
                            except Exception as e:
                                st.error(f"Failed to load sample: {str(e)}")
                
                # Display selected sample
                if 'selected_sample' in st.session_state:
                    sample_id = st.session_state['selected_sample']
                    if f'sample_{sample_id}_tensor' in st.session_state:
                        sample_tensor = st.session_state[f'sample_{sample_id}_tensor']
                        sample_label = st.session_state[f'sample_{sample_id}_label']
                        
                        original_image = tensor_to_image(sample_tensor)
                        st.image(original_image, 
                                caption=f"Sample {sample_id} (True: {data_loader.classes[sample_label]})", 
                                use_container_width=True)
                        input_tensor = sample_tensor.unsqueeze(0)
                
                # Alternative: Random sample generator
                st.write("Or generate a new random sample:")
                if st.button("ğŸ² Generate Random Sample", key="random_sample"):
                    try:
                        data_loaders = data_loader.get_loaders()
                        test_dataset = data_loaders['datasets']['test_clean']
                        
                        # Get a random sample
                        idx = np.random.randint(0, len(test_dataset))
                        sample_tensor, sample_label = test_dataset[idx]
                        
                        original_image = tensor_to_image(sample_tensor)
                        st.image(original_image, caption=f"Random Sample (True: {data_loader.classes[sample_label]})", use_container_width=True)
                        input_tensor = sample_tensor.unsqueeze(0)
                        
                    except Exception as e:
                        st.error(f"Failed to load sample: {str(e)}")
        
        elif input_method == "CIFAR-10 Test":
            if data_loader:
                test_type = st.selectbox("Test Set Type:", ["Clean", "Domain-Shifted"])
                
                if st.button("Get Random Test Image"):
                    try:
                        data_loaders = data_loader.get_loaders()
                        test_key = 'test_clean' if test_type == "Clean" else 'test_shifted'
                        test_dataset = data_loaders['datasets'][test_key]
                        
                        idx = np.random.randint(0, len(test_dataset))
                        sample_tensor, sample_label = test_dataset[idx]
                        
                        original_image = tensor_to_image(sample_tensor)
                        st.image(original_image, caption=f"Test Image ({test_type}) - True: {data_loader.classes[sample_label]}", use_container_width=True)
                        input_tensor = sample_tensor.unsqueeze(0)
                        
                    except Exception as e:
                        st.error(f"Failed to load test image: {str(e)}")
    
    with col2:
        st.subheader("ğŸ¤– AI Analysis")
        
        if input_tensor is not None:
            with st.spinner("Analyzing image..."):
                try:
                    # Move tensor to device
                    device = next(trust_model.model.parameters()).device
                    input_tensor = input_tensor.to(device)
                    
                    # Get predictions with trust analysis
                    results = trust_model.predict_with_trust(input_tensor)
                    
                    # Extract results
                    prediction = results['predictions'][0]
                    probabilities = results['probabilities'][0]
                    entropy = results['entropy'][0]
                    max_prob = results['max_probability'][0]
                    
                    # Get class names
                    if data_loader:
                        class_names = data_loader.classes
                        predicted_class = class_names[prediction]
                    else:
                        class_names = [f"Class {i}" for i in range(10)]
                        predicted_class = class_names[prediction]
                    
                    # Display prediction
                    st.success(f"**Predicted Class:** {predicted_class}")
                    st.info(f"**Confidence:** {max_prob:.3f}")
                    
                    # Trust score calculation
                    trust_score = trust_model.get_trust_score(
                        entropy,
                        results.get('mahalanobis_distance', [None])[0] if 'mahalanobis_distance' in results else None,
                        results.get('odin_entropy', [None])[0] if 'odin_entropy' in results else None
                    )
                    
                    # Trust level
                    if trust_score > 0.8:
                        trust_level = "HIGH"
                        trust_color = "trust-high"
                    elif trust_score > 0.5:
                        trust_level = "MEDIUM"
                        trust_color = "trust-medium"
                    else:
                        trust_level = "LOW"
                        trust_color = "trust-low"
                    
                    st.markdown(f'<div class="metric-card {trust_color}"><h4>Trust Level: {trust_level}</h4><p>Score: {trust_score:.3f}</p></div>', unsafe_allow_html=True)
                    
                    # Detailed metrics
                    st.subheader("ğŸ“Š Trust Metrics")
                    
                    metric_cols = st.columns(3)
                    
                    with metric_cols[0]:
                        st.metric("Entropy", f"{entropy:.3f}", help="Lower = more confident")
                    
                    with metric_cols[1]:
                        if 'odin_entropy' in results:
                            st.metric("ODIN Score", f"{results['odin_entropy'][0]:.3f}", help="OOD detection score")
                        else:
                            st.metric("ODIN Score", "N/A")
                    
                    with metric_cols[2]:
                        if 'mahalanobis_distance' in results:
                            st.metric("Mahal. Dist.", f"{results['mahalanobis_distance'][0]:.3f}", help="Distance to training distribution")
                        else:
                            st.metric("Mahal. Dist.", "N/A")
                    
                    # Probability distribution
                    st.subheader("ğŸ¯ Class Probabilities")
                    prob_df = pd.DataFrame({
                        'Class': class_names,
                        'Probability': probabilities
                    }).sort_values('Probability', ascending=False)
                    
                    st.bar_chart(prob_df.set_index('Class')['Probability'])
                    
                    # Top 3 predictions
                    st.subheader("ğŸ† Top 3 Predictions")
                    for i, (idx, row) in enumerate(prob_df.head(3).iterrows()):
                        rank_icon = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰"][i]
                        st.write(f"{rank_icon} **{row['Class']}**: {row['Probability']:.3f}")
                    
                except Exception as e:
                    st.error(f"Analysis failed: {str(e)}")
                    st.error(traceback.format_exc())
    
    # Full-width sections
    if input_tensor is not None:
          # Grad-CAM Explanation
        if use_gradcam and GRADCAM_AVAILABLE:
            st.subheader("ğŸ” Explainability: Grad-CAM")
            
            try:
                with st.spinner("Generating explanation..."):
                    gradcam_results = create_gradcam_for_model(trust_model.model, input_tensor)
                    
                    explanation_cols = st.columns(3)
                    
                    with explanation_cols[0]:
                        st.image(gradcam_results['original_image'], caption="Original", use_container_width=True)
                    
                    with explanation_cols[1]:
                        st.image(gradcam_results['heatmap'], caption="Attention Heatmap", use_container_width=True, clamp=True)
                    
                    with explanation_cols[2]:
                        st.image(gradcam_results['overlayed_image'], caption="Grad-CAM Overlay", use_container_width=True)
                    
                    st.info("ğŸ”¥ **Red areas** show regions that most influenced the model's decision.")
                    
            except Exception as e:
                st.error(f"Grad-CAM generation failed: {str(e)}")
          # Trust Dashboard
        st.subheader("ğŸ“ˆ Trust Dashboard")
        
        if 'results' in locals():
            # Create comprehensive trust metrics visualization
            dashboard_cols = st.columns([1, 1])
            
            with dashboard_cols[0]:
                # Trust gauge
                if 'trust_score' in locals():
                    gauge_fig = create_trust_gauge(trust_score, "Overall Trust Score")
                    st.plotly_chart(gauge_fig, use_container_width=True)
            
            with dashboard_cols[1]:
                # Detailed trust metrics with better formatting
                st.subheader("ğŸ¯ Trust Metrics Breakdown")
                
                # Entropy metric
                entropy_normalized = 1 - (entropy / np.log(10))  # Normalize for display
                st.metric(
                    label="ğŸ² Prediction Certainty", 
                    value=f"{entropy_normalized:.3f}",
                    help="Higher values indicate more certain predictions. Based on entropy calculation."
                )
                
                # ODIN metric
                if 'odin_entropy' in results and results['odin_entropy'][0] is not None:
                    odin_score = 1 - (results['odin_entropy'][0] / np.log(10))  # Normalize
                    st.metric(
                        label="ğŸš¨ OOD Detection Score", 
                        value=f"{odin_score:.3f}",
                        help="ODIN method score. Lower values suggest out-of-distribution samples."
                    )
                else:
                    st.metric(
                        label="ğŸš¨ OOD Detection Score", 
                        value="Not Available",
                        help="ODIN method requires gradient computation"
                    )
                
                # Mahalanobis distance metric
                if 'mahalanobis_distance' in results and results['mahalanobis_distance'][0] is not None:
                    mahal_dist = results['mahalanobis_distance'][0]
                    # Normalize for display (lower is better)
                    mahal_score = max(0, 1 - (mahal_dist / 100))  # Rough normalization
                    st.metric(
                        label="ğŸ“ Distribution Similarity", 
                        value=f"{mahal_score:.3f}",
                        help=f"Mahalanobis distance: {mahal_dist:.2f}. Higher values indicate closer match to training data."
                    )
                else:
                    st.metric(
                        label="ğŸ“ Distribution Similarity", 
                        value="Computing...",
                        help="Mahalanobis distance measures similarity to training distribution"
                    )
            
            # Additional trust information
            st.subheader("ğŸ“Š Trust Metrics Comparison")
            
            # Create a more informative bar chart
            trust_metrics = []
            metric_names = []
            metric_descriptions = []
            
            # Add entropy
            trust_metrics.append(1 - (entropy / np.log(10)))
            metric_names.append("Prediction Certainty")
            metric_descriptions.append("Based on entropy")
            
            # Add ODIN if available
            if 'odin_entropy' in results and results['odin_entropy'][0] is not None:
                trust_metrics.append(1 - (results['odin_entropy'][0] / np.log(10)))
                metric_names.append("ODIN Score")
                metric_descriptions.append("Out-of-distribution detection")
            
            # Add Mahalanobis if available
            if 'mahalanobis_distance' in results and results['mahalanobis_distance'][0] is not None:
                trust_metrics.append(max(0, 1 - (results['mahalanobis_distance'][0] / 100)))
                metric_names.append("Distribution Similarity")
                metric_descriptions.append("Mahalanobis distance")
            
            # Create DataFrame for better visualization
            if trust_metrics:
                trust_df = pd.DataFrame({
                    'Metric': metric_names,
                    'Score': trust_metrics,
                    'Description': metric_descriptions
                })
                
                # Use Streamlit's built-in bar chart with better formatting
                st.bar_chart(trust_df.set_index('Metric')['Score'])
                
                # Display table with details
                st.dataframe(
                    trust_df,
                    use_container_width=True,
                    hide_index=True
                )
    
    # Footer
    st.markdown("---")
    st.markdown("### About TrustLens")
    st.markdown("""
    TrustLens combines state-of-the-art uncertainty quantification and explainability techniques to provide trustworthy AI predictions:
    
    - ğŸ¯ **Entropy-based Uncertainty**: Measures prediction confidence using information theory
    - ğŸš¨ **ODIN Detection**: Identifies out-of-distribution inputs using input preprocessing
    - ğŸ“ **Mahalanobis Distance**: Detects anomalies in the feature space
    - ğŸ” **Grad-CAM**: Visualizes which parts of the image influenced the decision
    - ğŸšï¸ **Temperature Scaling**: Calibrates confidence scores for better reliability
    
    Built with â¤ï¸ for transparent and trustworthy AI.
    """)


if __name__ == "__main__":
    main()
